{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data\n",
    "Generate data in the tone of Larry David in a JSON format so that we could present it to the user at runtime with an emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt GPT-o to generate 100 conversation starters so we generate syntethic responses for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate conversation starters\n",
    "\n",
    "conversation_starter_generation_prompt = \"\"\"\n",
    "I want to interview Larry David and want to prompt him to say things that are\n",
    "very Larry David like. Give me 100 conversation starters in a JSON array of strings. \n",
    "Do not respond with anything else before or after the array.\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages = [{\n",
    "            \"role\":\"user\",\n",
    "            \"content\": conversation_starter_generation_prompt\n",
    "    }]\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-o generated 96 unique conversation starters\n"
     ]
    }
   ],
   "source": [
    "starters = json.loads(response.content)\n",
    "print(f\"GPT-o generated {len(list(set(starters)))} unique conversation starters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you feel about mandatory fun at company events?\n",
      "What’s your stance on public proposals?\n",
      "Is it just me, or is the concept of New Year's resolutions absurd?\n",
      "Should there be a rule about how many items one can bring into the express checkout line?\n",
      "Do you think it's odd when people call you ‘buddy’ or ‘pal’?\n"
     ]
    }
   ],
   "source": [
    "# print 5 random conversation starters\n",
    "for i in range(5):\n",
    "    print(random.choice(starters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Responses\n",
    "Set a system message to put GPT-4o in character and ask it to respond in our expected format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are Larry David. Speak like he does and take on his character. \n",
    "Be funny and sarcastic. When you respond, use a JSON format. \n",
    "Break your response into constituent sub-sentence parts of distinct \n",
    "emotionality and tone (merge consecutive \n",
    "parts that have the same emotionality). Give your responses in a JSON \n",
    "array in the form of:\n",
    "[{\"text\":\"...\", \"emotion\": \"e.g. happy, sad, neutral, excited, mad\"}]\n",
    "Do not respond with anything else before or after the array.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 out of 192...\n",
      "Generated 20 out of 192...\n",
      "Generated 30 out of 192...\n",
      "Generated 40 out of 192...\n",
      "Generated 50 out of 192...\n",
      "Generated 60 out of 192...\n",
      "Generated 70 out of 192...\n",
      "Generated 80 out of 192...\n",
      "Generated 90 out of 192...\n",
      "Generated 100 out of 192...\n",
      "Generated 110 out of 192...\n",
      "Generated 120 out of 192...\n",
      "Generated 130 out of 192...\n",
      "Generated 140 out of 192...\n",
      "Generated 150 out of 192...\n",
      "Generated 160 out of 192...\n",
      "Generated 170 out of 192...\n",
      "Generated 180 out of 192...\n",
      "Generated 190 out of 192...\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "examples = []\n",
    "\n",
    "responses_per_starter = 2\n",
    "\n",
    "messages_base = [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "total = len(starters) * responses_per_starter\n",
    "\n",
    "count = 0\n",
    "\n",
    "for starter in starters:\n",
    "    for i in range(responses_per_starter):\n",
    "        messages = messages_base + [{\"role\": \"user\", \"content\": starter}]\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages\n",
    "        )\n",
    "        response = completion.choices[0].message\n",
    "        examples.append(messages + [{\"role\": \"assistant\", \"content\": response.content}])\n",
    "        count += 1\n",
    "\n",
    "        if count % 10 == 0:\n",
    "            print(f\"Generated {count} out of {total}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are your thoughts on social norms around double-dipping at parties?\n",
      "[\n",
      "    {\"text\": \"Oh, you wanna talk about double-dipping?\", \"emotion\": \"amused\"},\n",
      "    {\"text\": \"It's like breaking some ancient, sacred rule.\", \"emotion\": \"sarcastic\"},\n",
      "    {\"text\": \"I mean, what's the big deal?\", \"emotion\": \"annoyed\"},\n",
      "    {\"text\": \"You dip once, you dip twice, who cares?\", \"emotion\": \"dismissive\"},\n",
      "    {\"text\": \"But, no, people freak out.\", \"emotion\": \"surprised\"},\n",
      "    {\"text\": \"They treat you like you're some sort of monster.\", \"emotion\": \"exasperated\"},\n",
      "    {\"text\": \"Oh no, Larry dipped twice, we’re all gonna die!\", \"emotion\": \"mocking\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(examples[3][1]['content'])\n",
    "print(examples[3][2]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Train and Validation Datasets (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation sets (80/20)\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(examples)\n",
    "split = int(0.8 * len(examples))\n",
    "train_data = examples[:split]\n",
    "valid_data = examples[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training and valid splits in jsonl files\n",
    "with open(\"larry_david_train.jsonl\", \"w\") as f:\n",
    "    for example in train_data:\n",
    "        f.write(json.dumps({\"messages\": example}) + \"\\n\")\n",
    "\n",
    "with open(\"larry_david_valid.jsonl\", \"w\") as f:\n",
    "    for example in valid_data:\n",
    "        f.write(json.dumps({\"messages\": example}) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Data to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = client.files.create(\n",
    "  file=open(\"larry_david_train.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_file = client.files.create(\n",
    "  file=open(\"larry_david_valid.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Fine-tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.fine_tuning.jobs.create(\n",
    "  training_file=train_file.id,\n",
    "  validation_file=valid_file.id,\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  suffix=\"larry_david_v3\",\n",
    "  seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-K2OChnZX23I29CAk9KEnk3Y4', created_at=1722398345, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-1106', object='fine_tuning.job', organization_id='org-2EBv1sMBw4eODiAsnJUJ6a9g', result_files=[], seed=42, status='validating_files', trained_tokens=None, training_file='file-70UC1z1H832NOL4deQIruKnV', validation_file='file-EGzEH3BN4h7fFW1uBMLYNqNR', estimated_finish=None, integrations=[], user_provided_suffix='larry_david_v3')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Trained Model on Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of model checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-3.5-turbo-1106:yoki-labs:larry-david-v3:9qv3WGdj:ckpt-step-153\n",
      "ft:gpt-3.5-turbo-1106:yoki-labs:larry-david-v3:9qv3W4QB:ckpt-step-306\n",
      "ft:gpt-3.5-turbo-1106:yoki-labs:larry-david-v3:9qv3Wzxp\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "    if \"larry-david-v3\" in model.id:\n",
    "        print(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def scrape_openai(model_id, output_file, validation_set):\n",
    "    model_scrape = []\n",
    "\n",
    "    for example in tqdm(validation_set):\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages = example[:2]\n",
    "        )\n",
    "        model_scrape.append(example[:2] + [{'role': 'assistant', 'content' : completion.choices[0].message.content}])\n",
    "    \n",
    "    # write model_scrape to a jsonl file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for example in model_scrape:\n",
    "            f.write(json.dumps({\"messages\": example}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_openai(\"gpt-3.5-turbo-1106-larry-david-v3\", \"larry_david_v3_scrape.jsonl\", valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:50<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "scrape_openai(\"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo_scrape.jsonl\", valid_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
